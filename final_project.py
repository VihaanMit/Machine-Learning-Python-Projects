# -*- coding: utf-8 -*-
"""Final_Project.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1eLBNaDUrDPEJv2WZpI_C5KHM7jW9Iky7
"""

"""
@author Vihaan Mittal
Final Project
"""
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.preprocessing import StandardScaler
from sklearn.preprocessing import Normalizer
import seaborn as sns
from pandas.plotting import scatter_matrix
from sklearn.linear_model import LinearRegression
from sklearn.feature_selection import RFE
from sklearn.model_selection import train_test_split, KFold, cross_val_score
from sklearn.metrics import mean_squared_error
from sklearn.linear_model import Lasso,Ridge
from sklearn.tree import DecisionTreeRegressor
from sklearn import metrics
import numpy as np
from sklearn.ensemble import RandomForestRegressor, AdaBoostRegressor
from sklearn.svm import SVR, LinearSVR
from sklearn.preprocessing import LabelEncoder
label_encoder = LabelEncoder()

column_names=["X1 transaction date","X2 house age","X3 distance to the nearest MRT station","X4 number of convenience stores","X5 latitude","X6 longitude"]
#Load data
dataset=pd.read_csv('Real estate.csv')

#Removing null rows
sumNullRws = dataset.isnull().sum()
dataset = dataset.dropna()

dataset=dataset.drop(columns=["No"])

# Split dataset into features and target
X = dataset.drop(columns=['Y house price of unit area'])
Y = dataset['Y house price of unit area']

#Print first 15 rows
print(dataset.head(15))

description=dataset.describe()
print(description)

df1=pd.DataFrame(dataset,columns=column_names)
df1.hist(column_names)
plt.show()

#Creating heatmap
plt.figure()
corMat = dataset.corr(method='pearson')
sns.heatmap(corMat, square=True)
plt.title("Correlation Heatmap")
plt.show()

#Creating scatterplot matrix
plt.figure()
scatter_matrix(dataset)
plt.show()

# Normalize the input features
scaler = Normalizer().fit(X)
X_normalized = scaler.transform(X)

# Create a DataFrame for the normalized features
X_normalized_df = pd.DataFrame(X_normalized, columns=X.columns)

# Combine the normalized features with the target variable
dataset_normalized = X_normalized_df.copy()
dataset_normalized['Y house price of unit area'] = Y.values

#Print first 15 rows
print(dataset_normalized.head(15))

#Descriptive Statistics of Normalized Data
description=dataset_normalized.describe()
print(description)

#Creating Normalized dataframe
df1=pd.DataFrame(dataset_normalized,columns=column_names)
df1.hist()
plt.show()

#Creating heatmap
plt.figure()
corMat = dataset_normalized.corr(method='pearson')
sns.heatmap(corMat, square=True)
plt.title("Correlation Heatmap")
plt.show()

#Creating scatterplot matrix
plt.figure()
scatter_matrix(dataset_normalized)
plt.show()

# Standardize the input features
scaler = StandardScaler().fit(X)
X_standardized = scaler.transform(X)

# Create a DataFrame for the standardized features
X_standardized_df = pd.DataFrame(X_standardized, columns=X.columns)

# Combine the standardized features with the target variable
dataset_standardized = X_standardized_df.copy()
dataset_standardized['Y house price of unit area'] = Y.values

#Print first 15 rows
print(dataset_standardized.head(15))

#Descriptive Statistics of Standardized Data
description=dataset_standardized.describe()
print(description)

#Creating Standardized dataframe
df1=pd.DataFrame(dataset_standardized,columns=column_names)
df1.hist()
plt.show()

#Creating heatmap
plt.figure()
corMat = dataset_standardized.corr(method='pearson')
sns.heatmap(corMat, square=True)
plt.title("Correlation Heatmap")
plt.show()

#Creating scatterplot matrix
plt.figure()
scatter_matrix(dataset_standardized)
plt.show()

#RFE
print("RFE")
NUM_FEATURES = 6
no_reg_mlr = LinearRegression()
rfe = RFE(estimator=no_reg_mlr, n_features_to_select=NUM_FEATURES)
fit = rfe.fit(X_normalized, Y)
print("Num Features:", fit.n_features_)
print("Selected Features:", fit.support_)
print("Feature Ranking:", fit.ranking_)
score = rfe.score(X_normalized, Y)
print("Model Score with selected features is:", score)

# Split data
X_train, X_test, Y_train, Y_test = train_test_split(X_normalized, Y, test_size=0.2, random_state=0)

# Initialize lists for models and results
names = []
models = []
results = []
linear_models=[]
linear_names=[]
linear_results=[]
scoring = 'neg_mean_squared_error'

# Selecting only the features that are supported by RFE
X_train_selected = X_train[:, fit.support_]
X_test_selected = X_test[:, fit.support_]

# Get training predictions and calculate MSE for non regularization
no_reg_mlr.fit(X_train_selected,Y_train)
Y_train_pred = no_reg_mlr.predict(X_train_selected)
train_mse = mean_squared_error(Y_train, Y_train_pred)
print('No regularization Train MSE:', train_mse)

# Fit the model using only the selected features
no_reg_mlr.fit(X_train_selected, Y_train)
y_pred = no_reg_mlr.predict(X_test_selected)
mse = mean_squared_error(Y_test, y_pred)
print("No Regularization Test MSE:", mse)

linear_models.append(no_reg_mlr)
linear_names.append('No regularization MLR')
models.append(no_reg_mlr)
names.append('No regularization MLR')

# Lasso
lasso = Lasso(alpha=1)

#RFE with Lasso
NUM_FEATURES = 6
model = lasso
rfe = RFE(estimator=lasso, n_features_to_select=NUM_FEATURES)
fit = rfe.fit(X_normalized, Y)
print("Num Features:", fit.n_features_)
print("Selected Features:", fit.support_)
print("Feature Ranking:", fit.ranking_)
score = rfe.score(X_normalized, Y)
print("Model Score with selected features is:", score)

# Get training MSE for lasso
lasso.fit(X_train_selected,Y_train)
Y_train_pred_lasso = lasso.predict(X_train_selected)
train_mse_lasso = mean_squared_error(Y_train, Y_train_pred_lasso)
print('Lasso Train MSE:', train_mse_lasso)

#Get testing MSE for Lasso
lasso.fit(X_train_selected, Y_train)
Y_pred_lasso = lasso.predict(X_test_selected)
mse_lasso = mean_squared_error(Y_test, Y_pred_lasso)
print('Lasso Test MSE:', mse_lasso)
linear_models.append(lasso)
linear_names.append('Lasso')
models.append(lasso)
names.append('Lasso')

# Ridge Regression with RFE
ridge = Ridge(alpha=1)

# RFE with Ridge
NUM_FEATURES = 2
rfe = RFE(estimator=ridge, n_features_to_select=NUM_FEATURES)
fit = rfe.fit(X_train, Y_train)
print("Num Features:", fit.n_features_)
print("Selected Features:", fit.support_)
print("Feature Ranking:", fit.ranking_)
score = rfe.score(X_train, Y_train)
print("Model Score with selected features is:", score)

# Selecting only the features that are supported by RFE
X_train_selected = X_train[:, fit.support_]
X_test_selected = X_test[:, fit.support_]

# Get training MSE for ridge
ridge.fit(X_train_selected,Y_train)
Y_train_pred_ridge = ridge.predict(X_train_selected)
train_mse_ridge = mean_squared_error(Y_train, Y_train_pred_ridge)
print('Ridge Train MSE:', train_mse_ridge)

# Fit the model using only the selected features
ridge.fit(X_train_selected, Y_train)
Y_pred_ridge = ridge.predict(X_test_selected)
mse_ridge = mean_squared_error(Y_test, Y_pred_ridge)
print('Ridge Test MSE:', mse_ridge)
linear_models.append(ridge)
linear_names.append('Ridge')
models.append(ridge)
names.append('Ridge')

# Linear models K-fold CV
print("Cross-Validation Results(Linear Models)")
i = 0
for linear_model in linear_models:
    kfold = KFold(n_splits=10, random_state=7, shuffle=True)
    cv_results = cross_val_score(linear_model, X_train_selected, Y_train, cv=kfold, scoring=scoring)
    linear_results.append(cv_results)
    msg = "%s: %f (%f)" % (linear_names[i], cv_results.mean(), cv_results.std())
    # Printing accuracy scores
    print(msg)
    i += 1

print("Train-Test Results(New models)")
#Decision Tree
dt_regressor = DecisionTreeRegressor(max_depth= 4)
dt_regressor.fit(X_train, Y_train)
y_pred = dt_regressor.predict(X_test)

# Get training MSE for Decision Tree
Y_train_pred_dt = dt_regressor.predict(X_train)
train_mse_dt = mean_squared_error(Y_train, Y_train_pred_dt)
print('Decision Tree Train MSE:', train_mse_dt)
print('Decision Tree Test MSE:', metrics.mean_squared_error(Y_test, y_pred))

models.append(dt_regressor)
names.append('Decision Tree')

# Random Forest Regression
random_forest = RandomForestRegressor(n_estimators=200, random_state=0)
random_forest.fit(X_train, Y_train)
Y_pred_rf = random_forest.predict(X_test)
rf_mse = mean_squared_error(Y_test, Y_pred_rf)
models.append(random_forest)
names.append('Random Forest')

# Get training MSE for Random Forest
Y_train_pred_rf = random_forest.predict(X_train)
train_mse_rf = mean_squared_error(Y_train, Y_train_pred_rf)
print('Random Forest Train MSE:', train_mse_rf)
print('Random Forest Test MSE:', metrics.mean_squared_error(Y_test, Y_pred_rf))

# AdaBoost Regression
ada_reg = AdaBoostRegressor(DecisionTreeRegressor(max_depth=1),n_estimators=200,learning_rate=0.5,random_state=0)
ada_reg.fit(X_train, Y_train)
Y_pred_ada = ada_reg.predict(X_test)
ada_mse = mean_squared_error(Y_test, Y_pred_ada)

# Get training MSE for AdaBoost
Y_train_pred_ada = ada_reg.predict(X_train)
train_mse_ada = mean_squared_error(Y_train, Y_train_pred_ada)

print('Adaboost Train MSE:', train_mse_ada)
print('Adaboost Test MSE:', metrics.mean_squared_error(Y_test, Y_pred_ada))

models.append(ada_reg)
names.append('Adaboost')

# Linear SVR
linear_svr = LinearSVR(random_state=0)
linear_svr.fit(X_train, Y_train)
Y_pred_linear_svr = linear_svr.predict(X_test)
linear_svr_mse = mean_squared_error(Y_test, Y_pred_linear_svr)
print('Linear SVR Test MSE:', linear_svr_mse)
models.append(linear_svr)
names.append('Linear SVR')

# Get training MSE for Linear SVR
Y_train_pred_linear_svr = linear_svr.predict(X_train)
train_mse_linear_svr = mean_squared_error(Y_train, Y_train_pred_linear_svr)
print('Linear SVR Train MSE:', train_mse_linear_svr)

# Kernel SVR
kernel_svr = SVR(kernel='rbf')
kernel_svr.fit(X_train, Y_train)
Y_pred_kernel_svr = kernel_svr.predict(X_test)
kernel_svr_mse = mean_squared_error(Y_test, Y_pred_kernel_svr)
print('Kernel SVR Test MSE:', kernel_svr_mse)
models.append(kernel_svr)
names.append('Kernel SVR')

# Get training MSE for Kernel SVR
Y_train_pred_kernel_svr = kernel_svr.predict(X_train)
train_mse_kernel_svr = mean_squared_error(Y_train, Y_train_pred_kernel_svr)
print('Kernel SVR Train MSE:', train_mse_kernel_svr)

# K-fold CV with all models
print("Cross-Validation Results(All Models)")
i = 0
for model in models:
    kfold = KFold(n_splits=10, random_state=7, shuffle=True)
    cv_results = cross_val_score(model, X_train_selected, Y_train, cv=kfold, scoring=scoring)
    results.append(cv_results)
    msg = "%s: %f (%f)" % (names[i], cv_results.mean(), cv_results.std())
    # Printing accuracy scores
    print(msg)
    i += 1

#Boxplot of K-fold results
fig = plt.figure()
fig.suptitle('Algorithm Comparison')
ax = fig.add_subplot(111)
plt.boxplot(results)
ax.set_xticklabels(names)
plt.show()

# Fitting MLR and Random Forest after K-fold

#MLR
model.fit(X_train_selected, Y_train)
y_pred = model.predict(X_test_selected)
mse = mean_squared_error(Y_test, y_pred)
print('No regularization MLR:',mse)

#Random Forest
random_forest.fit(X_train, Y_train)
Y_pred_rf = random_forest.predict(X_test)
rf_mse = mean_squared_error(Y_test, Y_pred_rf)
print('Random Forest:',rf_mse)