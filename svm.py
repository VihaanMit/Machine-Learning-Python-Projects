# -*- coding: utf-8 -*-
"""SVM.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1kEGTDB8R1VMieM8ECZ8q1j2KlJfO2jDY
"""

"""
@author Vihaan Mittal
SVM Assignment
"""

import numpy as np
import matplotlib.pyplot as plt
from itertools import cycle
from sklearn.datasets import load_wine
from sklearn.svm import SVC
from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics import auc, roc_curve, roc_auc_score, RocCurveDisplay
from sklearn.model_selection import train_test_split, KFold, cross_val_score
from sklearn.preprocessing import LabelBinarizer
from sklearn.multiclass import OneVsRestClassifier

# Load the wine dataset
wine = load_wine()
target_names = wine.target_names
X, y = wine.data, wine.target

# Split the data into train and test sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.5, stratify=y, random_state=0)

models = []
results = []
names = []
scoring = 'accuracy'

# Random forest classifier
rnd_clf_rf = RandomForestClassifier(max_depth=2, random_state=0)
models.append(rnd_clf_rf)
names.append('Random Forest')

# Train a OneVsRest SVC classifier
rnd_clf_ovr = OneVsRestClassifier(SVC(random_state=0, kernel='rbf', C=1.0, probability=True))
models.append(rnd_clf_ovr)
names.append('SVC')

# AdaBoost classifier
rnd_ada_clf = AdaBoostClassifier(DecisionTreeClassifier(max_depth=2), n_estimators=200, algorithm="SAMME.R", learning_rate=0.5, random_state=0)
models.append(rnd_ada_clf)
names.append('AdaBoost')

#K-fold CV
i = 0
for model in models:
    kfold = KFold(n_splits=10, random_state=7, shuffle=True)
    cv_results = cross_val_score(model, X, y, cv=kfold, scoring=scoring)
    results.append(cv_results)
    msg = "%s: %f (%f)" % (names[i], cv_results.mean(), cv_results.std())
    #Printing accuracy scores
    print(msg)
    i = i + 1

#Fitting models after k-fold
rnd_clf_ovr.fit(X_train, y_train)
y_prob_ovr = rnd_clf_ovr.predict_proba(X_test)

rnd_clf_rf.fit(X_train,y_train)
y_prob_rf = rnd_clf_rf.predict_proba(X_test)

rnd_ada_clf.fit(X_train,y_train)
y_prob_ada = rnd_ada_clf.predict_proba(X_test)

# Printing Boxplot
fig = plt.figure()
fig.suptitle('Algorithm Comparison')
ax = fig.add_subplot(111)
plt.boxplot(results)
ax.set_xticklabels(names)
plt.show()

# Printing ROC Curve for OnevsRest SVC

# Binarize the output
label_binarizer = LabelBinarizer().fit(y_train)
y_onehot_test = label_binarizer.transform(y_test)

# Store the fpr, tpr, and roc_auc for all classes
fpr = dict()
tpr = dict()
roc_auc = dict()
for i in range(len(target_names)):
    fpr[i], tpr[i], _ = roc_curve(y_onehot_test[:, i], y_prob_ovr[:, i])
    roc_auc[i] = auc(fpr[i], tpr[i])

# Compute the macro-average ROC curve and ROC area
fpr_grid = np.linspace(0.0, 1.0, 1000)
mean_tpr = np.zeros_like(fpr_grid)
for i in range(len(target_names)):
    mean_tpr += np.interp(fpr_grid, fpr[i], tpr[i])
mean_tpr /= len(target_names)

fpr["macro"] = fpr_grid
tpr["macro"] = mean_tpr
roc_auc["macro"] = auc(fpr["macro"], tpr["macro"])

# Compute the macro-average ROC AUC score using roc_auc_score
macro_roc_auc_ovr = roc_auc_score(y_onehot_test, y_prob_ovr, multi_class="ovr", average="macro")
print(f"Macro-averaged One-vs-Rest ROC AUC score (using roc_auc_score):\n{macro_roc_auc_ovr:.2f}")

# Plotting the ROC curves
fig, ax = plt.subplots(figsize=(6, 6))
plt.plot(fpr["macro"], tpr["macro"], label=f"macro-average ROC curve (AUC = {roc_auc['macro']:.2f})", color="navy", linestyle=":", linewidth=4)

colors = cycle(["aqua", "darkorange", "cornflowerblue"])
for i, color in zip(range(len(target_names)), colors):
    # Binarize y_test for the current class
    y_test_bin = label_binarizer.transform(y_test)[:, i]

    fpr[i], tpr[i], _ = roc_curve(y_test_bin, y_prob_ovr[:, i])

    RocCurveDisplay(fpr=fpr[i], tpr=tpr[i], roc_auc=roc_auc[i], estimator_name=f"ROC curve for {target_names[i]}").plot(ax=ax, name=f"ROC curve for {target_names[i]}", color=color)

ax.set(xlabel="False Positive Rate", ylabel="True Positive Rate", title="Extension of Receiver Operating Characteristic\nto One-vs-Rest multiclass")
plt.legend(loc="lower right")
plt.show()

# Printing ROC Curve for OnevsRest SVC

# Binarize the output
label_binarizer = LabelBinarizer().fit(y_train)
y_onehot_test = label_binarizer.transform(y_test)

# Store the fpr, tpr, and roc_auc for all classes
fpr = dict()
tpr = dict()
roc_auc = dict()
for i in range(len(target_names)):
    fpr[i], tpr[i], _ = roc_curve(y_onehot_test[:, i], y_prob_ada[:, i])
    roc_auc[i] = auc(fpr[i], tpr[i])

# Compute the macro-average ROC curve and ROC area
fpr_grid = np.linspace(0.0, 1.0, 1000)
mean_tpr = np.zeros_like(fpr_grid)
for i in range(len(target_names)):
    mean_tpr += np.interp(fpr_grid, fpr[i], tpr[i])
mean_tpr /= len(target_names)

fpr["macro"] = fpr_grid
tpr["macro"] = mean_tpr
roc_auc["macro"] = auc(fpr["macro"], tpr["macro"])

# Compute the macro-average ROC AUC score using roc_auc_score
macro_roc_auc_ada = roc_auc_score(y_onehot_test, y_prob_ada, multi_class="ovr", average="macro")
print(f"Macro-averaged One-vs-Rest ROC AUC score (using roc_auc_score):\n{macro_roc_auc_ada:.2f}")

# Plotting the ROC curves
fig, ax = plt.subplots(figsize=(6, 6))
plt.plot(fpr["macro"], tpr["macro"], label=f"macro-average ROC curve (AUC = {roc_auc['macro']:.2f})", color="navy", linestyle=":", linewidth=4)

colors = cycle(["aqua", "darkorange", "cornflowerblue"])
for i, color in zip(range(len(target_names)), colors):
    # Binarize y_test for the current class
    y_test_bin = label_binarizer.transform(y_test)[:, i]

    fpr[i], tpr[i], _ = roc_curve(y_test_bin, y_prob_ada[:, i])

    RocCurveDisplay(fpr=fpr[i], tpr=tpr[i], roc_auc=roc_auc[i], estimator_name=f"ROC curve for {target_names[i]}").plot(ax=ax, name=f"ROC curve for {target_names[i]}", color=color)

ax.set(xlabel="False Positive Rate", ylabel="True Positive Rate", title="Extension of Receiver Operating Characteristic\nto One-vs-Rest multiclass")
plt.legend(loc="lower right")
plt.show()

# Printing ROC Curve for OnevsRest SVC

# Binarize the output
label_binarizer = LabelBinarizer().fit(y_train)
y_onehot_test = label_binarizer.transform(y_test)

# Store the fpr, tpr, and roc_auc for all classes
fpr = dict()
tpr = dict()
roc_auc = dict()
for i in range(len(target_names)):
    fpr[i], tpr[i], _ = roc_curve(y_onehot_test[:, i], y_prob_rf[:, i])
    roc_auc[i] = auc(fpr[i], tpr[i])

# Compute the macro-average ROC curve and ROC area
fpr_grid = np.linspace(0.0, 1.0, 1000)
mean_tpr = np.zeros_like(fpr_grid)
for i in range(len(target_names)):
    mean_tpr += np.interp(fpr_grid, fpr[i], tpr[i])
mean_tpr /= len(target_names)

fpr["macro"] = fpr_grid
tpr["macro"] = mean_tpr
roc_auc["macro"] = auc(fpr["macro"], tpr["macro"])

# Compute the macro-average ROC AUC score using roc_auc_score
macro_roc_auc_rf = roc_auc_score(y_onehot_test, y_prob_rf, average="macro")
print(f"Macro-averaged One-vs-Rest ROC AUC score (using roc_auc_score):\n{macro_roc_auc_rf:.2f}")

# Plotting the ROC curves
fig, ax = plt.subplots(figsize=(6, 6))
plt.plot(fpr["macro"], tpr["macro"], label=f"macro-average ROC curve (AUC = {roc_auc['macro']:.2f})", color="navy", linestyle=":", linewidth=4)

colors = cycle(["aqua", "darkorange", "cornflowerblue"])
for i, color in zip(range(len(target_names)), colors):
    # Binarize y_test for the current class
    y_test_bin = label_binarizer.transform(y_test)[:, i]

    fpr[i], tpr[i], _ = roc_curve(y_test_bin, y_prob_rf[:, i])

    RocCurveDisplay(fpr=fpr[i], tpr=tpr[i], roc_auc=roc_auc[i], estimator_name=f"ROC curve for {target_names[i]}").plot(ax=ax, name=f"ROC curve for {target_names[i]}", color=color)

ax.set(xlabel="False Positive Rate", ylabel="True Positive Rate", title="Extension of Receiver Operating Characteristic\nto One-vs-Rest multiclass")
plt.legend(loc="lower right")
plt.show()